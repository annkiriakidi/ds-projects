{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В цьому домашньому завданні ми знову працюємо з даними з нашого змагання [\"Bank Customer Churn Prediction (DLU Course)\"](https://www.kaggle.com/t/7c080c5d8ec64364a93cf4e8f880b6a0).\n",
        "\n",
        "Тут ми побудуємо рішення задачі класифікації з використанням алгоритмів бустингу: XGBoost та LightGBM, а також використаємо бібліотеку HyperOpt для оптимізації гіперпараметрів."
      ],
      "metadata": {
        "id": "fDefDHQt8LXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Зчитайте дані `train.csv` в змінну `raw_df` та скористайтесь наведеним кодом нижче аби розділити дані на трнувальні та валідаційні і розділити дані на ознаки з матириці Х та цільову змінну. Назви змінних `train_inputs, train_targets, train_inputs, train_targets` можна змінити на ті, які Вам зручно.\n",
        "\n",
        "  Наведений скрипт - частина отриманого мною скрипта для обробки даних. Ми тут не викнуємо масштабування та обробку категоріальних змінних, бо хочемо це делегувати алгоритмам, які будемо використовувати. Якщо щось не розумієте в наведених скриптах, рекомендую розібратись: навичка читати код - важлива складова роботи в машинному навчанні."
      ],
      "metadata": {
        "id": "LhivzW9W8-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from hyperopt import hp, fmin, tpe, Trials\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "o3cNXkXhIsBs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "\n",
        "def split_train_val(df: pd.DataFrame, target_col: str, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the dataframe into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw dataframe.\n",
        "        target_col (str): The target column for stratification.\n",
        "        test_size (float): The proportion of the dataset to include in the validation split.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: Training and validation dataframes.\n",
        "    \"\"\"\n",
        "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_col])\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def separate_inputs_targets(df: pd.DataFrame, input_cols: list, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Separate inputs and targets from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe.\n",
        "        input_cols (list): List of input columns.\n",
        "        target_col (str): Target column.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: DataFrame of inputs and Series of targets.\n",
        "    \"\"\"\n",
        "    inputs = df[input_cols].copy()\n",
        "    targets = df[target_col].copy()\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "cKE8RTPf6CRD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "raw_df = raw_df.drop(columns=[\"CustomerId\", \"Surname\"])\n",
        "\n",
        "categorical_cols = [\"Geography\", \"Gender\"]\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    raw_df[col] = le.fit_transform(raw_df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "def split_train_val(df, target_col, test_size=0.2, random_state=42):\n",
        "    return train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_col])\n",
        "\n",
        "target_col = \"Exited\"\n",
        "train_df, val_df = split_train_val(raw_df, target_col)\n",
        "\n",
        "input_cols = [col for col in raw_df.columns if col != target_col]\n",
        "train_inputs, train_targets = train_df[input_cols], train_df[target_col]\n",
        "val_inputs, val_targets = val_df[input_cols], val_df[target_col]\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_clf.fit(train_inputs, train_targets)\n",
        "xgb_preds = xgb_clf.predict(val_inputs)\n",
        "xgb_acc = accuracy_score(val_targets, xgb_preds)\n",
        "\n",
        "lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "lgb_clf.fit(train_inputs, train_targets)\n",
        "lgb_preds = lgb_clf.predict(val_inputs)\n",
        "lgb_acc = accuracy_score(val_targets, lgb_preds)\n",
        "\n",
        "def objective_xgb(params):\n",
        "    clf = xgb.XGBClassifier(**params, random_state=42)\n",
        "    clf.fit(train_inputs, train_targets)\n",
        "    preds = clf.predict(val_inputs)\n",
        "    return -accuracy_score(val_targets, preds)\n",
        "\n",
        "space_xgb = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    'max_depth': hp.choice('max_depth', [3, 5, 7]),\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "best_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=20, trials=trials)\n",
        "\n",
        "print(\"XGBoost Accuracy:\", xgb_acc)\n",
        "print(\"LightGBM Accuracy:\", lgb_acc)\n",
        "print(\"Best XGBoost params:\", best_xgb)\n"
      ],
      "metadata": {
        "id": "-bHdMJVB4xQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b37cc4-9d4b-4ac4-86a1-e6bcebdf1adc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1096\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "100%|██████████| 20/20 [00:09<00:00,  2.04trial/s, best loss: -0.9]\n",
            "XGBoost Accuracy: 0.8946666666666667\n",
            "LightGBM Accuracy: 0.8963333333333333\n",
            "Best XGBoost params: {'learning_rate': 0.19145472794096244, 'max_depth': 0, 'n_estimators': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. В тренувальному та валідаційному наборі перетворіть категоріальні ознаки на тип `category`. Можна це зробити двома способами:\n",
        " 1. `df[col_name].astype('category')`, як було продемонстровано в лекції\n",
        " 2. використовуючи метод `pd.Categorical(df[col_name])`"
      ],
      "metadata": {
        "id": "cq0JU7MqHgp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [\"Geography\", \"Gender\"]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    train_df[col] = pd.Categorical(train_df[col])\n",
        "    val_df[col] = pd.Categorical(val_df[col])"
      ],
      "metadata": {
        "id": "UPmqo-Mr4yUO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Навчіть на отриманих даних модель `XGBoostClassifier`. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів XGBoostClassifier - тут https://xgboost.readthedocs.io/en/stable/parameter.html#global-config\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування `XGBoostClassifier` аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Можна також, якщо працюєте в Google Colab, увімкнути можливість використання GPU (`Runtime -> Change runtime type -> T4 GPU`) і встановити параметр `device='cuda'` в `XGBoostClassifier` для пришвидшення тренування бустинг моделі.\n",
        "  \n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням DecisionTrees раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "_LxWkv4o-wMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    tree_method='hist',\n",
        "    enable_categorical=True,\n",
        "    missing=np.nan,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_clf.fit(train_inputs, train_targets)\n",
        "\n",
        "train_preds_proba = xgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = xgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(\"Train AUROC:\", train_auc)\n",
        "print(\"Validation AUROC:\", val_auc)"
      ],
      "metadata": {
        "id": "_5rDqdDP41hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d879e454-9646-427c-8db9-d5820c0c0c1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train AUROC: 0.9730805964327622\n",
            "Validation AUROC: 0.9316510048700184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки**\n",
        "\n",
        "Баланс між bias і variance виглядає добре."
      ],
      "metadata": {
        "id": "0BLwxKbgryve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `XGBoostClassifier` з лекції знайдіть оптимальні значення гіперпараметрів `XGBoostClassifier` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **20**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. В ній ми маємо задати loss - це може будь-яка метрика, але бажано використовувтаи ту, яка цільова в вашій задачі. Чим менший лосс - тим ліпша модель на думку hyperopt. Тож, тут нам треба задати loss - негативне значення AUROC. В лекції ми натомість використовували Accuracy.\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_clf` модель `XGBoostClassifier` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_clf`\n",
        "    - оцініть якість моделі `final_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (2) цього завдання?"
      ],
      "metadata": {
        "id": "U4hm5qYs_f7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "space_xgb = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    'max_depth': hp.choice('max_depth', [3, 5, 7]),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
        "}\n",
        "\n",
        "def objective_xgb(params):\n",
        "    clf = xgb.XGBClassifier(\n",
        "        **params,\n",
        "        use_label_encoder=False,\n",
        "        tree_method='hist',\n",
        "        enable_categorical=True,\n",
        "        missing=np.nan,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(train_inputs, train_targets)\n",
        "    preds_proba = clf.predict_proba(val_inputs)[:, 1]\n",
        "    return -roc_auc_score(val_targets, preds_proba)\n",
        "\n",
        "trials = Trials()\n",
        "best_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=20, trials=trials)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_xgb)\n",
        "\n",
        "final_clf = xgb.XGBClassifier(\n",
        "    n_estimators=[50, 100, 200][best_xgb['n_estimators']],\n",
        "    learning_rate=best_xgb['learning_rate'],\n",
        "    max_depth=[3, 5, 7][best_xgb['max_depth']],\n",
        "    subsample=best_xgb['subsample'],\n",
        "    colsample_bytree=best_xgb['colsample_bytree'],\n",
        "    use_label_encoder=False,\n",
        "    tree_method='hist',\n",
        "    enable_categorical=True,\n",
        "    missing=np.nan,\n",
        "    random_state=42\n",
        ")\n",
        "final_clf.fit(train_inputs, train_targets)\n",
        "\n",
        "train_preds_proba = final_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = final_clf.predict_proba(val_inputs)[:, 1]\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(\"Train AUROC:\", train_auc)\n",
        "print(\"Validation AUROC:\", val_auc)"
      ],
      "metadata": {
        "id": "WhR1g9B4433r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781a9426-66a0-465a-e118-336c35f3d697"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  5%|▌         | 1/20 [00:00<00:03,  5.07trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 10%|█         | 2/20 [00:00<00:04,  4.48trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 20%|██        | 4/20 [00:00<00:03,  4.67trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 30%|███       | 6/20 [00:01<00:02,  4.69trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 40%|████      | 8/20 [00:01<00:02,  5.32trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 50%|█████     | 10/20 [00:01<00:01,  5.86trial/s, best loss: -0.9353467316002468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 60%|██████    | 12/20 [00:02<00:01,  6.42trial/s, best loss: -0.93589546608135]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 70%|███████   | 14/20 [00:02<00:00,  6.39trial/s, best loss: -0.9361417106797447]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 80%|████████  | 16/20 [00:02<00:00,  7.26trial/s, best loss: -0.9361417106797447]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 85%|████████▌ | 17/20 [00:02<00:00,  6.74trial/s, best loss: -0.9361417106797447]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 95%|█████████▌| 19/20 [00:03<00:00,  5.35trial/s, best loss: -0.936620481514507] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 20/20 [00:03<00:00,  5.52trial/s, best loss: -0.936620481514507]\n",
            "Best Hyperparameters: {'colsample_bytree': 0.5116545453548452, 'learning_rate': 0.05507032544050008, 'max_depth': 0, 'n_estimators': 2, 'subsample': 0.6727346304084919}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:35:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train AUROC: 0.9433537715081971\n",
            "Validation AUROC: 0.936620481514507\n",
            "Баланс між bias і variance виглядає добре\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки.**\n",
        "\n",
        "Баланс між bias і variance виглядає добре. Але самі показники трохи впали 😢"
      ],
      "metadata": {
        "id": "6AQA9kcisivA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Навчіть на наших даних модель LightGBM. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів LightGBM - тут https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування LightGBM аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Аби передати категоріальні колонки в LightGBM - необхідно виявити їх індекси і передати в параметрі `cat_feature=cat_feature_indexes`\n",
        "\n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням XGBoostClassifier раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "Vg77SVWrBBmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(raw_df, test_size=0.2, random_state=42, stratify=raw_df[target_col])\n",
        "\n",
        "cat_feature_indexes = [train_df.columns.get_loc(col) for col in categorical_cols]\n",
        "\n",
        "input_cols = [col for col in raw_df.columns if col != target_col]\n",
        "train_inputs, train_targets = train_df[input_cols], train_df[target_col]\n",
        "val_inputs, val_targets = val_df[input_cols], val_df[target_col]\n",
        "\n",
        "lgb_clf = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    boosting_type='gbdt',\n",
        "    random_state=42\n",
        ")\n",
        "lgb_clf.fit(train_inputs, train_targets, categorical_feature=cat_feature_indexes)\n",
        "\n",
        "train_preds_proba = lgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = lgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(\"Train AUROC:\", train_auc)\n",
        "print(\"Validation AUROC:\", val_auc)"
      ],
      "metadata": {
        "id": "C-9aZn4d45No",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a7b635-6e2f-4db8-bfca-c331fdc058f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "Train AUROC: 0.975844531400087\n",
            "Validation AUROC: 0.9325752795116263\n",
            "Баланс між bias і variance виглядає добре\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки**\n",
        "\n",
        "Train AUROC вдалось трохи підтягнути\n"
      ],
      "metadata": {
        "id": "KcyjY9tqNO-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `LightGBM` з лекції знайдіть оптимальні значення гіперпараметрів `LightGBM` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **10**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. І тут ми також ставимо loss - негативне значення AUROC, як і при пошуці гіперпараметрів для XGBoost. До речі, можна спробувати написати код так, аби в objective передавати лише модель і не писати схожий код двічі :)\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_lgb_clf` модель `LightGBM` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_lgb_clf`\n",
        "    - оцініть якість моделі `final_lgb_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (4) цього завдання?"
      ],
      "metadata": {
        "id": "nCnkGD_sEW1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_lgb(params):\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        **params,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(train_inputs, train_targets, categorical_feature=cat_feature_indexes)\n",
        "    preds_proba = clf.predict_proba(val_inputs)[:, 1]\n",
        "    return -roc_auc_score(val_targets, preds_proba)\n",
        "\n",
        "space_lgb = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    'num_leaves': hp.choice('num_leaves', [20, 31, 50]),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "best_lgb = fmin(fn=objective_lgb, space=space_lgb, algo=tpe.suggest, max_evals=10, trials=trials)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_lgb)\n",
        "\n",
        "final_lgb_clf = lgb.LGBMClassifier(\n",
        "    n_estimators=[50, 100, 200][best_lgb['n_estimators']],\n",
        "    learning_rate=best_lgb['learning_rate'],\n",
        "    num_leaves=[20, 31, 50][best_lgb['num_leaves']],\n",
        "    subsample=best_lgb['subsample'],\n",
        "    colsample_bytree=best_lgb['colsample_bytree'],\n",
        "    random_state=42\n",
        ")\n",
        "final_lgb_clf.fit(train_inputs, train_targets, categorical_feature=cat_feature_indexes)\n",
        "\n",
        "train_preds_proba = final_lgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = final_lgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(\"Train AUROC:\", train_auc)\n",
        "print(\"Validation AUROC:\", val_auc)"
      ],
      "metadata": {
        "id": "cfMQKA4D47Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb48790-7af0-4af9-e350-85ece6edc8e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001153 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001351 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003460 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001333 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002084 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.12trial/s, best loss: -0.9363001577611633]\n",
            "Best Hyperparameters: {'colsample_bytree': 0.7583442321213254, 'learning_rate': 0.03609858659415139, 'n_estimators': 1, 'num_leaves': 0, 'subsample': 0.5346089782251953}\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 843\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "Train AUROC: 0.9462170610946505\n",
            "Validation AUROC: 0.9363001577611633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки**\n",
        "\n",
        "Результати знову трохи впали\n"
      ],
      "metadata": {
        "id": "Z3N2MzWRNo3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Оберіть модель з експериментів в цьому ДЗ і зробіть новий `submission` на Kaggle та додайте код для цього і скріншот скора на публічному лідерборді.\n",
        "  \n",
        "  **Напишіть коментар, чому ви обрали саме цю модель?**\n",
        "\n",
        "  І я вас вітаю - це останнє завдання з цим набором даних 💪 На цьому етапі корисно проаналізувати, які моделі показали себе найкраще і подумати, чому."
      ],
      "metadata": {
        "id": "XArADR2CG8VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "submission_ids = test_df[\"id\"]\n",
        "\n",
        "test_df = test_df.drop(columns=[\"id\", \"CustomerId\", \"Surname\"])\n",
        "\n",
        "test_df[\"Geography\"] = test_df[\"Geography\"].astype(\"category\")\n",
        "test_df[\"Gender\"] = test_df[\"Gender\"].astype(\"category\")\n",
        "\n",
        "submission_preds = final_lgb_clf.predict_proba(test_df)[:, 1]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": submission_ids,\n",
        "    \"Exited\": submission_preds\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "COIjJH9f5SSp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Знімок екрана 2025-03-07 о 12.51.28.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "9PVSrvPEMw11"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KR9hPXJMzrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}